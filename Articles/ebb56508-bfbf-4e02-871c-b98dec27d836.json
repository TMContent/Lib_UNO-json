{
  "TeamMentor_Article": {
    "$": {
      "Metadata_Hash": "0",
      "Content_Hash": "0"
    },
    "Metadata": [
      {
        "Id": [
          "ebb56508-bfbf-4e02-871c-b98dec27d836"
        ],
        "Id_History": [
          "ebb56508-bfbf-4e02-871c-b98dec27d836,da24a97c-2e28-4573-9164-d3de798e7e5c,"
        ],
        "Library_Id": [
          "26bd1a04-beed-4a66-917d-b6ab0a7d634c"
        ],
        "Title": [
          "Search Engines Are Blocked from Sensitive Areas"
        ],
        "Category": [
          "Hardening"
        ],
        "Phase": [
          "Deployment"
        ],
        "Technology": [
          "Technology Independent"
        ],
        "Type": [
          "Checklist Item"
        ],
        "DirectLink": [
          "Search Engines Are Blocked from Sensitive Areas"
        ],
        "Author": [
          ""
        ],
        "Priority": [
          ""
        ],
        "Status": [
          ""
        ]
      }
    ],
    "Content": [
      {
        "$": {
          "Sanitized": "false",
          "DataType": "wikitext"
        },
        "Data": [
          "==Applies To==\r\n\r\n* Web Applications\r\n\r\n\r\n==What to Check For==\r\n\r\nMake sure that search engines are blocked from indexing sensitive areas on your site.\r\n\r\n\r\n==Why==\r\n\r\nIf a search engine indexes sensitive data on your web site, an attacker will be able to retrieve that data from the search engine.\r\n\r\n\r\n==How to Check==\r\n\r\nTo make sure that search engines are blocked from indexing sensitive areas on your site:\r\n\r\n# **Identify sensitive URI locations.** Enumerate all the URIs that lead to sensitive data in your application. These URIs will usually correspond to the filesystem structure of the application, but sometimes they will be produced dynamically by the application itself. Whatever the case, consider how the URIs look from the perspective of the client when enumerating the locations of the sensitive data. It is strongly recommended to prevent indexing of any administrative pages.\r\n# **Examine the robots.txt file.** Make sure that the robots.txt file is present in the web root directory of the server and that is web accessible. Use a web browser to check if the robots.txt file is available at /robots.txt URI on your site. Review the robots.txt file to make sure that it includes rules that block search engines from indexing each of the sensitive URIs on your site.\r\n# **Examine search engine caches.** Use popular web search engines to execute search queries that include sensitive URI locations on your server.  See if these locations have already been cached. If sensitive data has already been cached, it should be purged automatically some time later when the search engine crawls your site again and parses the robots.txt file correctly. If that is not acceptable, consider contacting the search engine operator and asking them to remove the sensitive data from their caches.\r\n\r\n\r\n==How to Fix==\r\n\r\nTo block search engines from indexing sensitive data on your site by using a robots.txt file:\r\n\r\n# **Identify sensitive URI locations.** Enumerate all the URIs that lead to sensitive data in your application. These URIs will usually correspond to the filesystem structure of the application, but sometimes they will be produced dynamically by the application itself. Whatever the case, consider how the URIs look from the perspective of the client when enumerating the locations of the sensitive data. It is strongly recommended to prevent indexing of any administrative pages.\r\n# **Write a robots.txt file.** The robots.txt file has a simple syntax. Study the reference at the link at the bottom of this page, and write a robots.txt file that prevents indexing of all the sensitive URI locations.\r\n# **Upload the robots.txt file to the web site.** Upload the robots.txt file to the web root directory on your application server. \r\n# **(Optional) Examine search engine caches.** Use popular web search engines to execute search queries that include sensitive URI locations on your server. See if these locations have already been cached. If sensitive data has already been cached, it should be purged automatically some time later when the search engine crawls your site again and parses the robots.txt file correctly. If that is not acceptable, consider contacting the search engine operator and asking them to remove the sensitive data from their caches.\r\n\r\n\r\n==Additional Resources==\r\n\r\n* For information about the robots.txt file syntax, see http://www.w3.org/TR/html4/appendix/notes.html#h-B.4.1.1\r\n"
        ]
      }
    ]
  }
}